{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc5fec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\artem\\anaconda3\\envs\\generative-ai\\Lib\\site-packages\\controlnet_aux\\mediapipe_face\\mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline, StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler, StableDiffusionControlNetPipeline\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from controlnet_aux import CannyDetector\n",
    "import torch, PIL.Image as Image\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a48666",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    'lllyasviel/sd-controlnet-canny', torch_dtype=torch.float16\n",
    ")\n",
    "canny = CannyDetector()\n",
    "image = Image.open('dragon.jpg').convert('RGB')\n",
    "control_image = canny(image)\n",
    "control_image.save('canny_output.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e36c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001c23cac44e4ba59c1e63df0714f760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    }
   ],
   "source": [
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    'stable-diffusion-v1-5/stable-diffusion-v1-5', controlnet=controlnet, torch_dtype=torch.float16\n",
    ").to('cuda')\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# LoRA адаптація\n",
    "pipe.load_lora_weights('BJEon121/trained-sd1.5_fancy_boot', weights_name='pytorch_lora_weights.safetensors')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5941e63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac8a97131d5413998553c4c63942402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = 'a majestic dragon sitting on the mountain, watercolor style'\n",
    "image = pipe(prompt, image=control_image, guidance_scale=9, num_inference_steps=30).images[0]\n",
    "image.save('final_result.png')\n",
    "\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e366a",
   "metadata": {},
   "source": [
    "# Порівняння"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b66f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'a majestic dragon sitting on the mountain, watercolor style'\n",
    "\n",
    "canny = CannyDetector()\n",
    "input_image = Image.open('dragon.jpg').convert('RGB')\n",
    "control_image = canny(input_image)\n",
    "control_image.save('canny_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0080e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e40a88cb5b142ac9f35ea1ded90fe69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83e6cbb195c4e77a013de232e5ce3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Базовий пайплайн (тільки prompt)\n",
    "pipe_base = StableDiffusionPipeline.from_pretrained(\n",
    "    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
    "    torch_dtype=torch.float16\n",
    ").to('cuda')\n",
    "pipe_base.scheduler = UniPCMultistepScheduler.from_config(pipe_base.scheduler.config)\n",
    "\n",
    "img_base = pipe_base(prompt, guidance_scale=9, num_inference_steps=30).images[0]\n",
    "img_base.save('result_base_prompt.png')\n",
    "\n",
    "del pipe_base\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95eae09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c0057f5a6e4f7f8d5bf892c4545c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0364281f5e7a4362aa81158fe664cdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Пайплайн з ControlNet\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    'lllyasviel/sd-controlnet-canny', torch_dtype=torch.float16\n",
    ")\n",
    "pipe_controlnet = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16\n",
    ").to('cuda')\n",
    "pipe_controlnet.scheduler = UniPCMultistepScheduler.from_config(pipe_controlnet.scheduler.config)\n",
    "\n",
    "img_controlnet = pipe_controlnet(prompt, image=control_image, guidance_scale=9, num_inference_steps=30).images[0]\n",
    "img_controlnet.save('result_prompt_controlnet.png')\n",
    "\n",
    "del pipe_controlnet\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59eabb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2193557f7a3415c87729209c1cc5c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178c08a32ca8443d910d54b0c33aa4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Базовий пайплайн + LoRA\n",
    "pipe_lora = StableDiffusionPipeline.from_pretrained(\n",
    "    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
    "    torch_dtype=torch.float16\n",
    ").to('cuda')\n",
    "pipe_lora.scheduler = UniPCMultistepScheduler.from_config(pipe_lora.scheduler.config)\n",
    "pipe_lora.load_lora_weights('BJEon121/trained-sd1.5_fancy_boot', weight_name='pytorch_lora_weights.safetensors')\n",
    "\n",
    "img_lora = pipe_lora(prompt, guidance_scale=9, num_inference_steps=30).images[0]\n",
    "img_lora.save('result_prompt_lora.png')\n",
    "\n",
    "del pipe_lora\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aea8e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a04b1622944ef1be251196b9350fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42431c1ed6141e08ef13c538a65dbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Пайплайн з ControlNet + LoRA\n",
    "pipe_full = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    'stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16\n",
    ").to('cuda')\n",
    "pipe_full.scheduler = UniPCMultistepScheduler.from_config(pipe_full.scheduler.config)\n",
    "pipe_full.load_lora_weights('BJEon121/trained-sd1.5_fancy_boot', weight_name='pytorch_lora_weights.safetensors')\n",
    "\n",
    "img_full = pipe_full(prompt, image=control_image, guidance_scale=9, num_inference_steps=30).images[0]\n",
    "img_full.save('result_prompt_lora_controlnet.png')\n",
    "\n",
    "del pipe_full\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6376e470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0ea66",
   "metadata": {},
   "source": [
    "**1. У чому полягає суть адаптації LoRA?**  \n",
    "\n",
    "LoRA (Low-Rank Adaptation) — це техніка донавчання великих моделей, яка дозволяє адаптувати поведінку моделі, додаючи невелику кількість нових параметрів замість повного перенавчання всіх ваг. Суть полягає в тому, що LoRA вносить зміни лише у певні шари (зазвичай cross-attention), вставляючи матриці низького рангу, що суттєво зменшує обсяг необхідної пам'яті та часу на навчання. Завдяки цьому модель зберігає оригінальні знання, одночасно навчаючись новим завданням або стилям.\n",
    "\n",
    "**2. Які типи ControlNet можна використовувати?**  \n",
    "\n",
    "ControlNet підтримує різні типи умовних вхідних даних, що формують структуру або стиль майбутнього зображення. Найпоширеніші типи включають: Canny-контури (крайові карти), Depth maps (карти глибини), Pose estimation (скелетні пози людини), Scribbles (ескізи), Segmentation maps (семантична сегментація), Normal maps (орієнтація поверхні) та OpenPose (детальне розпізнавання тіла). Кожен тип ControlNet допомагає більш точно керувати тим, що і як буде генеруватися.\n",
    "\n",
    "**3. Як працює cross-attention в контексті LoRA і ControlNet?**  \n",
    "Cross-attention — це механізм, за допомогою якого генеративна модель під час створення зображення враховує умови, такі як текстовий опис або контрольне зображення. У випадку LoRA модифікуються саме cross-attention шари, щоб легше пристосувати модель до нових стилів або специфічних тематик без повного перенавчання. ControlNet також інтегрується через cross-attention, де додаються додаткові контрольні сигнали, що впливають на розподіл уваги моделі між текстовим описом і структурною підказкою.\n",
    "\n",
    "**4. У чому переваги поєднання тексту і зображення як умов?**  \n",
    "Поєднання текстових і візуальних умов дозволяє досягати одночасно креативності й точності: текст визначає зміст, атмосферу та художній стиль сцени, а зображення (наприклад, контур або поза) забезпечує фізичну структуру й композицію. Таке комбіноване управління дає змогу створювати реалістичні сцени, відповідні обом вимогам одночасно, що важко реалізувати за допомогою лише одного типу умови.\n",
    "\n",
    "**5. Які існують обмеження або потенційні проблеми при комбінуванні LoRA та ControlNet?**  \n",
    "Комбінація LoRA і ControlNet може призводити до кількох викликів: збільшене споживання пам’яті (оскільки обидва методи додають додаткові шари/параметри), можливий конфлікт між стилем (LoRA) і структурою (ControlNet), що ускладнює досягнення бажаного результату без тонкої настройки. Також важливо правильно балансувати ваги впливу, інакше або LoRA буде \"перебивати\" структуру ControlNet, або навпаки — ControlNet буде занадто обмежувати творчість LoRA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
